# -
yolov5实现人车目标检测
## 人车识别检测

##### 20204460233

##### 比赛网址


```
目标检测在自动驾驶等方面有着广泛的应用前景，在自动
驾驶场景中，使用视觉传感器对人和车目标进行检测并规避障
碍物。
此挑战旨在使用Oneflow框架对真实场景下行车记录仪采集
图片的人和车目标进行检测，在 2 D目标检测的设置下对人和车
进行目标检测。
```
### 赛题任务：

```
此赛题的数据集由云测数据提供。比赛数据集中包含
3000 张真实场景下行车记录仪采集的图片，其中训练集
包含 2600 张带有标签的图片，测试集包含 400 张不带有标
签的图片。
参赛者需基于Oneflow框架在训练集上进行训练，对
测试集中两种类型的对象：人和车进行两点目标检测，并
对检测的目标进行识别。
数据集中共有 22 种细分的人车类型标签。
```

### 原始赛题数据：

##### train文件夹

- train图片文件夹
- json文件

##### test文件夹

- test图片文件夹


### json内部格式：


### coco 2 yolo:

###### 由于本次我使用yolov 5 来训练，需要将coco（json）的文件转

###### 成yolo格式（txt）

###### 提取json内容 并将其写成txt与其图片相对应

###### yolo格式需要txt文件中的内容是 label xywh

###### 而json文件给出的是xyxy格式的数据，而且没有给出图片的长

###### 和宽

###### 这就需要我来将每一个图片导进来，然后通过img.shape方法

###### 来得到长和宽，再将xyxy转成xywh

###### label 我通过字典的方式 将每个label 对应的数字写入txt文件中


在划分数据集的时候我进行了很多方法的试验，刚开始把json转成txt然后转成csv表格，发现这样太麻烦了而且最后划分
的结果不太好然后我就继续尝试其他方法

```
总共数据集划分写了 7 个文件，发现最后用到的只有两个
```
```
而且最后这个方法还可以直接将对应label和图片分开存储
```
### 划分数据集：


从图像文件夹中随机抽取 10 %比例的图像，并根据图像名字，抽取对应的txt标签，将其作为


最后我再将图片和label按照yolo格式来放置


### 修改yolo配置文件：

### 从网页上下载预训练权重:


### 修改train：


### 运行train：

```
我在本地运行发现容易把电脑烧坏，我把迭代次
数改成了五十，在google云gpu上来跑，从晚上
11 点半跑到凌晨 2 点班，才训练完。
```

### 训练结果得到两个权重：

```
best权重是 50 次迭代中最好的权重
last权重是最后一次迭代得到的权重
```
```
我在后边的预测中使用的是best.pt
```

### 训练结果： 混淆矩阵：


置信度和F 1 分数的关系图


第一个图是训练集得数据量，每个类别多少
第二个就是labels
第三个就是center xy
第四个就是label得高宽


这个也是关于标记框的统计图


准确率和置信度的关系图


PR曲线中的P代表的是
precision（精准率），
R代表的是recall（召回
率），其代表的是精
准率与召回率的关系，
一般情况下，将recall
设置为横坐标，
precision设置为纵坐标。
PR曲线下围成的面积
即AP，所有类别AP平
均值即Map

如果其中一个学习器的
PR曲线A完全包住另一
个学习器B的PR曲线，
则可断言A的性能优于B。
但是A和B发生交叉，那
性能该如何判断呢？我
们可以根据曲线下方的
面积大小来进行比较，
但更常用的是平衡点F 1 。
平衡点（BEP）是P=R
时的取值（斜率为 1 ），
F 1 值越大，我们可以认
为该学习器的性能较好。




Box：YOLO V 5 使用 GIOU Loss作为bounding box的损失，Box推测为GIoU损失函数均值，越小方框越准；
Objectness：推测为目标检测loss均值，越小目标检测越准；
Classification：推测为分类loss均值，越小分类越准；
Precision：精度（找对的正类/所有找到的正类）；在这里插入图片描述
翻译成中文就是“分类器认为是正类并且确实是正类的部分占所有分类器认为是正类的比例”，衡量的是一个分类器分出
来的正类的确是正类的概率。两种极端情况就是，如果精度是 100 %，就代表所有分类器分出来的正类确实都是正类。如
果精度是 0 %，就代表分类器分出来的正类没一个是正类。光是精度还不能衡量分类器的好坏程度，比如 50 个正样本和 50
个负样本，我的分类器把 49 个正样本和 50 个负样本都分为负样本，剩下一个正样本分为正样本，这样我的精度也是 100 %，
但是显然这个分类器不太行。

```
Recall：召回率（找对的正类/所有本应该被找对的正类）；
在这里插入图片描述
翻译成中文就是“分类器认为是正类并且确实是正类的部分占所有确实是正类的比例”，衡量的是一个分类能把所有的正
类都找出来的能力。两种极端情况，如果召回率是 100 %，就代表所有的正类都被分类器分为正类。如果召回率是 0 %，就
代表没一个正类被分为正类。
```
```
mAP@ 0. 5 & mAP@ 0. 5 : 0. 95 ：就是mAP是用Precision和Recall作为两轴作图后围成的面积，m表示平均，@后面的数表示
判定iou为正负样本的阈值，@ 0. 5 : 0. 95 表示阈值取 0. 5 : 0. 05 : 0. 95 后取均值。
```
```
一般训练结果主要观察精度和召回率波动情况（波动不是很大则训练效果较好）
然后观察mAP@ 0. 5 & mAP@ 0. 5 : 0. 95 评价训练结果。
```

### 修改detect配置：

```
使用best权重
conf是置信度阈值：只显示预测概率超过conf_thres的预测结果
iou是非极大抑制 ：删除重复框，保留置信度分数最大的框。
```

### detect运行过程：


### detect运行结果：


### 将结果生成json文件：


### 成绩及排名：


# 后边是重磅内容

# ！！！！！！！


### 过程中遇到的困难以及解决方案：

```
看我自己写的一些用于划分数据集以及格式转
换的文件，就可以知道我进行了很多种方法的
尝试，当时真的搞得我非常迷茫。最后自己写
一个把前边代码综合一下，没想到成功了。
```
```
还有就是我刚开始格式转换的时候没有把
box那个写进去，就只有label，后续我发现
人家需要这个，然后我就重新搞把box写进
去，最后发现我的detect没有画出框来，我
调整了conf和iou也不行，最后我发现我的
train的结果里它的框都在图片外边了，我才
发现是我的box写错了，我又重新改了一遍，
改的时候我发现我们没有图片的宽高，只能
一张张地读取图片获取图片的宽和高。
```

### 训练过程中的问题：

```
我刚开始在自己电脑上跑，因为不知道迭代什么的东西，直接就跑，
发现好慢，然后我就改成一次让它先跑，成功了，然后我就改成
20 次发现对于我的电脑损害太大，然后我就询问了学长，学长告
诉我要用Google的云gpu，我就开始去学习这个云gpu，最后我找
到了colab，colab与Google云端硬盘相连接，先把自己的项目传到
Google云硬盘，再导入colab，修改一些路径什么的就可以运行了，
我将迭代次数改成 50 次，从晚上 11 点半跑到了凌晨两点半。我觉
得这个速度还是有点慢，我就又去学习了一下amazon的这个
SageMaker Studio Lab，这个东西需要先申请，申请通过后才可以
使用，我最近两天才申请通过所以就没有用这个来跑，不过可以以
后来用。
```

### 总结：

```
本次课设让我收获颇丰，我学习了
```
- pytorch
- anaconda
- yolov 5
- coco 2 yolo
- xyxy 2 xywh
- xywh 2 xyxy
- 数据集划分
- 数据平衡
- opencv
- google云gpu
- amazon云gpu
- txt转csv
- json转txt
- json转csv
- git的使用（git clone git push git pull）


### 最后我附上我的github地址：

#### github地址


